{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ca85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import ujson as json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4accd",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008adbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"tweets_#gohawks.txt\", \"tweets_#gopatriots.txt\", \n",
    "             \"tweets_#nfl.txt\", \"tweets_#patriots.txt\", \n",
    "             \"tweets_#sb49.txt\", \"tweets_#superbowl.txt\"]\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de26fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(file):\n",
    "    with open(\"ECE219_tweet_data/\" + file) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    total_followers = 0\n",
    "    total_retweets = 0\n",
    "    lines_all = []\n",
    "    max_date = 0\n",
    "    min_date = float('inf')\n",
    "    for i in lines:\n",
    "        temp = json.loads(i)\n",
    "        lines_all.append(temp)\n",
    "        total_followers += temp[\"author\"][\"followers\"]\n",
    "        total_retweets += temp[\"metrics\"][\"citations\"][\"total\"]\n",
    "        if temp[\"citation_date\"] < min_date:\n",
    "            min_date = temp[\"citation_date\"]\n",
    "        if temp[\"citation_date\"] > max_date:\n",
    "            max_date = temp[\"citation_date\"]\n",
    "        \n",
    "    #max_date = max(lines_all, key = lambda x:x[\"citation_date\"])[\"citation_date\"]\n",
    "    #min_date = min(lines_all, key = lambda x:x[\"citation_date\"])[\"citation_date\"]\n",
    "    avg_tweets_hour = len(lines_all) / (max_date - min_date)\n",
    "    avg_tweets_hour *= 60 * 60 \n",
    "    avg_followers_tweet = total_followers / len(lines_all)\n",
    "    avg_retweets_tweet = total_retweets / len(lines_all)\n",
    "    \n",
    "    metrics[file] = [avg_tweets_hour, avg_followers_tweet, avg_retweets_tweet, max_date, min_date]\n",
    "    return lines_all\n",
    "\n",
    "def get_hours(file):\n",
    "    lines = tweet_dict[file]\n",
    "    #max_date = max(lines, key = lambda x:x[\"citation_date\"])[\"citation_date\"]\n",
    "    #min_date = min(lines, key = lambda x:x[\"citation_date\"])[\"citation_date\"]\n",
    "    max_date = metrics[file][-2]\n",
    "    min_date = metrics[file][-1]\n",
    "    diff = max_date - min_date\n",
    "    hours = 60 * 60\n",
    "    which_hour = [np.floor((i[\"citation_date\"] - min_date) / hours) + 1 for i in lines]\n",
    "    count = Counter(which_hour)\n",
    "    df = pd.DataFrame.from_dict(count, orient = 'index')\n",
    "    return df.reset_index().rename(columns = {0: \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57fd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadcee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tweets_#gohawks.txt\n",
      "Beginning tweets_#gopatriots.txt\n",
      "Beginning tweets_#nfl.txt\n"
     ]
    }
   ],
   "source": [
    "for i in filenames[:3]:\n",
    "    print(\"Beginning\", i)\n",
    "    tweet_dict[i] = get_metrics(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0aa791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict[3] = get_metrics(filenames[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7be811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict[4] = get_metrics(filenames[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict[5] = get_metrics(filenames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48447ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics.index = [\"avg_tweets_hour\", \"avg_follower_tweet\", \"avg_retweets_tweet\"]\n",
    "df_metrics = df_metrics.T\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nfl = get_hours(\"tweets_#nfl.txt\")\n",
    "df_sb = get_hours(\"tweets_#superbowl.txt\")\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "ax[0].bar(range(len(df_nfl)), df_nfl[\"count\"])\n",
    "ax[0].set_xlabel(\"Hours\")\n",
    "ax[0].set_ylabel(\"Number of tweets\")\n",
    "ax[0].set_title(\"Tweets per Hour\" + \"#nfl.txt\")\n",
    "\n",
    "ax[1].bar(range(len(df_sb)), df_sb[\"count\"])\n",
    "ax[1].set_xlabel(\"Hours\")\n",
    "ax[1].set_ylabel(\"Number of tweets\")\n",
    "ax[1].set_title(\"Tweets per Hour\" + \"#sb.txt\")\n",
    " \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74844fca",
   "metadata": {},
   "source": [
    "# Design Our Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58edaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "for keys, vals in tweet_dict.items(): #different files \n",
    "    for j in vals: #vals in list of dictionaries, j is individual tweets\n",
    "        print(keys, vals)\n",
    "        tweet_f = {\"tweet\": j[\"tweet\"][\"text\"], \n",
    "                   \"hashtags\": keys.split(\"_\")[1][1:-4], #[k[\"text\"] for k in j[\"tweet\"][\"entities\"][\"hashtags\"]], \n",
    "                   \"retweets\": j[\"metrics\"][\"citations\"][\"total\"], \n",
    "                   \"time\": j[\"citation_date\"]}\n",
    "        tweet_data.append(tweet_f)\n",
    "        \n",
    "tweet_df = pd.DataFrame(tweet_data) #entire data set to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf4fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d80585",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    #Remove hashtags from text\n",
    "    text = \" \".join([word for word in text.split() if \"#\" not in word])\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    # Lemmatize the text\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # Get the sentiment scores\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    # Add the sentiment scores to the text\n",
    "    text = text + ' ' + str(sentiment_scores['neg']) + ' ' + str(sentiment_scores['neu']) + ' ' + str(sentiment_scores['pos'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0538c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processed_text will contain lemmatized as well as negative, neural, and positive scores\n",
    "tweet_df[\"no_hashtags\"] = tweet_df[\"tweet\"].apply(remove_hashtags)\n",
    "tweet_df[\"processed_text_no_hash\"] = tweet_df[\"no_hashtags\"].apply(preprocess_text) \n",
    "tweet_df[\"processed_text_w_hash\"] = tweet_df[\"tweet\"].apply(preprocess_text) \n",
    "\n",
    "feat_cols_rt = \"processed_text_w_hash\"\n",
    "feat_cols_ht = \"process_text_no_hash\"\n",
    "targ_col_rt = \"retweets\"\n",
    "targ_col_ht = \"hashtags\"\n",
    "train_rt, test_rt = train_test_split(tweet_df[[feat_cols_rt] + [targ_col_rt]], test_size = 0.2)\n",
    "train_ht, test_ht = train_test_split(tweet_df[[feat_cols_ht] + [targ_col_ht]], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorization + tfidf transformer\n",
    "pipe = Pipeline([(\"count\", CountVectorizer(stop_words = \"english\", min_df = 3)), \n",
    "                 (\"tfid\", TfidfTransformer())]) \n",
    "\n",
    "train_rt_pipe = pipe.fit_transform(train_rt[feat_cols_rt]) #fit pipeline and transform with train set\n",
    "test_rt_pipe = pipe.transform(test_rt[feat_cols_rt]) #only transform test set\n",
    "\n",
    "train_ht_pipe = pipe.fit_transform(train_ht[feat_cols_ht]) #fit pipeline and transform with train set\n",
    "test_ht_pipe = pipe.transform(test_ht[feat_cols_ht]) #only transform test set\n",
    "\n",
    "#Latent Semantic Indexing \n",
    "#retweets\n",
    "lsi_mod = TruncatedSVD(n_components = 1000, random_state = 0)\n",
    "train_rt_lsi = lsi_mod.fit_transform(train_rt_pipe)\n",
    "test_rt_lsi = lsi_mod.transform(test_rt_pipe)\n",
    "lsi_rt_mse = LA.norm(train_rt_pipe - np.dot(train_rt_lsi, lsi_mod.components_)) ** 2\n",
    "\n",
    "#hashtags\n",
    "train_ht_lsi = lsi_mod.fit_transform(train_ht_pipe)\n",
    "test_ht_lsi = lsi_mod.transform(test_ht_pipe)\n",
    "lsi_ht_mse = LA.norm(train_ht_pipe - np.dot(train_ht_lsi, lsi_mod.components_)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53835a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean-Sqaured Error for the Retweet LSI is\", lsi_rt_mse)\n",
    "print(\"Mean-Sqaured Error for the Hashtag LSI is\", lsi_ht_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a06230",
   "metadata": {},
   "source": [
    "# Neural Net Predicting Number of Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886540a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting Retweets Using MLP\n",
    "\n",
    "mlp = MLPRegressor(max_iter = 1000, verbose = False)\n",
    "hidden_layer_sizes = [50, 75, 100]\n",
    "layer_sizes = []\n",
    "for i in range(1, len(hidden_layer_sizes) + 1):\n",
    "    for subset in itertools.combinations_with_replacement(hidden_layer_sizes, i):\n",
    "        layer_sizes += [list(subset)]\n",
    "        \n",
    "#layer_sizes = np.random.choice(layer_sizes, size = 15, replace = False)\n",
    "\n",
    "param = {\"hidden_layer_sizes\": layer_sizes, \n",
    "         \"activation\": [\"relu\", \"identity\", \"logistic\", \"tanh\"], \n",
    "         \"alpha\": [10.0 ** x for x in np.arange(-3 , 3)]}\n",
    "\n",
    "X = train_rt_lsi\n",
    "y = train_rt[targ_col_rt]\n",
    "grid_mlp_twt = GridSearchCV(estimator = mlp, \n",
    "                            param_grid = param, \n",
    "                            cv = 10, \n",
    "                            scoring = \"neg_mean_squared_error\", \n",
    "                            n_jobs = -1)\n",
    "grid_mlp_twt.fit(X, y)\n",
    "df_mlp_twt = pd.DataFrame({\"Best MLP\": [grid_mlp_twt.best_estimator_, \n",
    "                                    grid_mlp_twt.best_params_, \n",
    "                                    np.sqrt(-1 * grid_mlp_twt.best_score_)]})\n",
    "df_mlp_twt.index = [\"Best Estimator\", \"Best Parameters\", \"RMSE\"]\n",
    "df_mlp_twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_twt = grid_mlp_twt.best_estimator_\n",
    "best_mlp_twt.fit(X, y)\n",
    "y_pred = grid_mlp_twt.predict(test_rt_lsi)\n",
    "mlp_rmse_rt = mean_squared_error(test_rt[targ_col_rt], y_pred, squared = False)\n",
    "print(\"The best MLP Model is\", best_mlp_twt)\n",
    "print(\"With the test set, the RMSE is\", mlp_rmse_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944877",
   "metadata": {},
   "source": [
    "# Predicting Hashtags using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting Hashtags Using Random Forest\n",
    "rf_twt = RandomForestClassifier(oob_score = True)\n",
    "param = {\"max_depth\": [1, 2, 3, 4, 5], \n",
    "         \"n_estimators\": [100, 200, 500, 1000, 2000]}\n",
    "grid_rf_twt = GridSearchCV(estimator = rf, \n",
    "                           param_grid = param, \n",
    "                           cv = 10, \n",
    "                           scoring = \"accuracy\", \n",
    "                           n_jobs = -1)\n",
    "X = train_ht_lsi\n",
    "y = train_ht[targ_col_ht]\n",
    "df_rf_twt = pd.DataFrame({\"Best Random Forest\": [grid_rf.best_estimator_, \n",
    "                                                 grid_rf.best_params_, \n",
    "                                                 grid_rf.best_score_, \n",
    "                                                 grid_rf.best_estimator_.oob_score_]})\n",
    "df_rf_twt.index = [\"Best Estimator\", \"Best Parameters\", \"Accuracy\", \"OOB Score\"]\n",
    "df_rf_twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_twt = grid_rf_twt.best_estimator_\n",
    "best_rf_twt.fit(X, y)\n",
    "y_pred = grid_rf_twt.predict(test_ht_lsi)\n",
    "rf_acc_twt = accuracy_score(test[targ_col_ht], y_pred)\n",
    "print(\"The best Random Forest Classifier Model is\", best_rf_twt)\n",
    "print(\"The OOB score is\", grid_rf.best_estimator_.oob_score_)\n",
    "print(\"The test accuracyis\", rf_acc_twt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7f7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ce214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67569a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a024b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04914d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7471f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
